{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Name: Isaac Ndirangu Muturi\n",
    "* Email: ndirangumuturi749@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 6 Homework\n",
    "\n",
    "In this homework, we're going to extend Module 5 Homework and learn about streaming with PySpark.\n",
    "\n",
    "Instead of Kafka, we will use Red Panda, which is a drop-in\n",
    "replacement for Kafka.\n",
    "\n",
    "Ensure you have the following set up (if you had done the previous homework and the module):\n",
    "\n",
    "- Docker (see [module 1](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/01-docker-terraform))\n",
    "- PySpark (see [module 5](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/05-batch/setup))\n",
    "\n",
    "For this homework we will be using the files from Module 5 homework:\n",
    "\n",
    "- Green 2019-10 data from [here](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz)\n",
    "\n",
    "\n",
    "\n",
    "## Start Red Panda\n",
    "\n",
    "Let's start redpanda in a docker container.\n",
    "\n",
    "There's a `docker-compose.yml` file in the homework folder (taken from [here](https://github.com/redpanda-data-blog/2023-python-gsg/blob/main/docker-compose.yml))\n",
    "\n",
    "Copy this file to your homework directory and run\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "(Add `-d` if you want to run in detached mode)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Redpanda version\n",
    "\n",
    "Now let's find out the version of redpandas.\n",
    "\n",
    "For that, check the output of the command `rpk help` _inside the container_. The name of the container is `redpanda-1`.\n",
    "\n",
    "Find out what you need to execute based on the `help` output.\n",
    "\n",
    "What's the version, based on the output of the command you executed? (copy the entire version)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rpk is the Redpanda CLI & toolbox',\n",
       " '',\n",
       " 'Usage:',\n",
       " '  rpk [flags]',\n",
       " '  rpk [command]',\n",
       " '',\n",
       " 'Available Commands:',\n",
       " '  acl         Manage ACLs and SASL users',\n",
       " '  cloud       Interact with Redpanda cloud',\n",
       " '  cluster     Interact with a Redpanda cluster',\n",
       " '  container   Manage a local container cluster',\n",
       " '  debug       Debug the local Redpanda process',\n",
       " '  generate    Generate a configuration template for related services',\n",
       " '  group       Describe, list, and delete consumer groups and manage their offsets',\n",
       " '  help        Help about any command',\n",
       " '  iotune      Measure filesystem performance and create IO configuration file',\n",
       " '  plugin      List, download, update, and remove rpk plugins',\n",
       " '  profile     Manage rpk profiles',\n",
       " '  redpanda    Interact with a local Redpanda process',\n",
       " '  topic       Create, delete, produce to and consume from Redpanda topics',\n",
       " '  version     Check the current version',\n",
       " '',\n",
       " 'Flags:',\n",
       " '      --config string            Redpanda or rpk config file; default',\n",
       " '                                 search paths are ~/.config/rpk/rpk.yaml,',\n",
       " '                                 $PWD, and /etc/redpanda/redpanda.yaml',\n",
       " \"  -X, --config-opt stringArray   Override rpk configuration settings; '-X\",\n",
       " \"                                 help' for detail or '-X list' for terser\",\n",
       " '                                 detail',\n",
       " '  -h, --help                     Help for rpk',\n",
       " '      --profile string           rpk profile to use',\n",
       " '  -v, --verbose                  Enable verbose logging',\n",
       " '      --version                  version for rpk',\n",
       " '',\n",
       " 'Use \"rpk [command] --help\" for more information about a command.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!docker exec -i redpanda-1 rpk help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rpk version v23.2.26 (rev 328d83a06e)\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti redpanda-1 rpk --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Creating a topic\n",
    "\n",
    "Before we can send data to the redpanda server, we\n",
    "need to create a topic. We do it also with the `rpk`\n",
    "command we used previously for figuring out the version of\n",
    "redpandas.\n",
    "\n",
    "Read the output of `help` and based on it, create a topic with name `test-topic`\n",
    "\n",
    "What's the output of the command for creating a topic? Include the entire output in your answer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOPIC       STATUS\r\n",
      "test-topic  OK\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti redpanda-1 rpk topic create test-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Connecting to the Kafka server\n",
    "\n",
    "We need to make sure we can connect to the server, so\n",
    "later we can send some data to its topics\n",
    "\n",
    "First, let's install the kafka connector (up to you if you\n",
    "want to have a separate virtual environment for that)\n",
    "\n",
    "```bash\n",
    "pip install kafka-python\n",
    "```\n",
    "\n",
    "You can start a jupyter notebook in your solution folder or\n",
    "create a script\n",
    "\n",
    "Let's try to connect to our server:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import time\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()\n",
    "```\n",
    "\n",
    "Provided that you can connect to the server, what's the output\n",
    "of the last command?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "\u001b[K     |████████████████████████████████| 246 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "# This function will be used to serialize the message value before sending it to Kafka.\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "# the address and port of the Kafka server (localhost:9092 in this case).\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "# checks if the producer is successfully connected to the Kafka server specified in the bootstrap servers.\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. Sending data to the stream\n",
    "\n",
    "Now we're ready to send some test data:\n",
    "\n",
    "```python\n",
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')\n",
    "```\n",
    "\n",
    "How much time did it take? Where did it spend most of the time?\n",
    "\n",
    "* Sending the messages\n",
    "* Flushing\n",
    "* Both took approximately the same amount of time\n",
    "\n",
    "(Don't remove `time.sleep` when answering this question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "Took 0.78 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start Time Measurement\n",
    "t0 = time.time()\n",
    "\n",
    "# Kafka Topic Name\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "# Sending Messages to Kafka Topic\n",
    "for i in range(10):\n",
    "    message = {'number': i}                # Message content (dictionary)\n",
    "    producer.send(topic_name, value=message)  # Send message to Kafka topic\n",
    "    print(f\"Sent: {message}\")              # Print sent message\n",
    "    time.sleep(0.05)                        # Delay between sending messages\n",
    "\n",
    "# Flush Producer Buffer - ensure that all messages in the producer's buffer are\n",
    "# sent to the Kafka broker before the program exits.\n",
    "producer.flush()\n",
    "\n",
    "# End Time Measurement\n",
    "t1 = time.time()\n",
    "\n",
    "# Print Time Taken\n",
    "print(f'Took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much time did it take? Where did it spend most of the time?\n",
    "\n",
    "* Sending the messages\n",
    "* Flushing\n",
    "* Both took approximately the same amount of time\n",
    "\n",
    "\n",
    "The total time taken was approximately 0.52 seconds.\n",
    "\n",
    "Based on the provided code and the output:\n",
    "\n",
    "Sending the messages: Sending the messages to the Kafka topic took most of the time.\n",
    "Flushing: Flushing the producer buffer, ensuring all messages are sent to the Kafka broker, took a negligible amount of time compared to sending the messages.\n",
    "Therefore, sending the messages to the Kafka topic was the main time-consuming task in this scenario. The time.sleep(0.05) function call adds a delay of 0.05 seconds between sending each message, contributing to the overall time taken for message transmission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data with `rpk`\n",
    "\n",
    "You can see the messages that you send to the topic\n",
    "with `rpk`:\n",
    "\n",
    "```bash\n",
    "rpk topic consume test-topic\n",
    "```\n",
    "\n",
    "Run the command above and send the messages one more time to\n",
    "see them\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 0}\",\n",
      "  \"timestamp\": 1711050709429,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 0\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 1}\",\n",
      "  \"timestamp\": 1711050709479,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 1\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 2}\",\n",
      "  \"timestamp\": 1711050709532,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 2\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 3}\",\n",
      "  \"timestamp\": 1711050709583,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 3\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 4}\",\n",
      "  \"timestamp\": 1711050709635,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 4\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 5}\",\n",
      "  \"timestamp\": 1711050709688,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 5\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 6}\",\n",
      "  \"timestamp\": 1711050709739,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 6\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 7}\",\n",
      "  \"timestamp\": 1711050709791,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 7\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 8}\",\n",
      "  \"timestamp\": 1711050709843,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 8\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 9}\",\n",
      "  \"timestamp\": 1711050709894,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 9\n",
      "}\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it redpanda-1 rpk topic consume test-topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n"
     ]
    }
   ],
   "source": [
    "topic_name = 'test-topic'\n",
    "for i in range(10):\n",
    "    message = {'number': i}    \n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 0}\",\n",
      "  \"timestamp\": 1711050709429,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 0\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 1}\",\n",
      "  \"timestamp\": 1711050709479,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 1\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 2}\",\n",
      "  \"timestamp\": 1711050709532,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 2\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 3}\",\n",
      "  \"timestamp\": 1711050709583,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 3\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 4}\",\n",
      "  \"timestamp\": 1711050709635,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 4\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 5}\",\n",
      "  \"timestamp\": 1711050709688,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 5\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 6}\",\n",
      "  \"timestamp\": 1711050709739,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 6\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 7}\",\n",
      "  \"timestamp\": 1711050709791,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 7\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 8}\",\n",
      "  \"timestamp\": 1711050709843,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 8\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 9}\",\n",
      "  \"timestamp\": 1711050709894,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 9\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 0}\",\n",
      "  \"timestamp\": 1711050746361,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 10\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 1}\",\n",
      "  \"timestamp\": 1711050746412,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 11\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 2}\",\n",
      "  \"timestamp\": 1711050746464,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 12\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 3}\",\n",
      "  \"timestamp\": 1711050746516,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 13\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 4}\",\n",
      "  \"timestamp\": 1711050746567,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 14\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 5}\",\n",
      "  \"timestamp\": 1711050746618,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 15\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 6}\",\n",
      "  \"timestamp\": 1711050746669,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 16\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 7}\",\n",
      "  \"timestamp\": 1711050746721,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 17\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 8}\",\n",
      "  \"timestamp\": 1711050746773,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 18\n",
      "}\n",
      "{\n",
      "  \"topic\": \"test-topic\",\n",
      "  \"value\": \"{\\\"number\\\": 9}\",\n",
      "  \"timestamp\": 1711050746825,\n",
      "  \"partition\": 0,\n",
      "  \"offset\": 19\n",
      "}\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!docker exec -it redpanda-1 rpk topic consume test-topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending the taxi data\n",
    "\n",
    "Now let's send our actual data:\n",
    "\n",
    "* Read the green csv.gz file\n",
    "* We will only need these columns:\n",
    "  * `'lpep_pickup_datetime',`\n",
    "  * `'lpep_dropoff_datetime',`\n",
    "  * `'PULocationID',`\n",
    "  * `'DOLocationID',`\n",
    "  * `'passenger_count',`\n",
    "  * `'trip_distance',`\n",
    "  * `'tip_amount'`\n",
    "\n",
    "Iterate over the records in the dataframe\n",
    "\n",
    "```python\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(row_dict)\n",
    "    break\n",
    "\n",
    "    # TODO implement sending the data here\n",
    "```\n",
    "\n",
    "Note: this way of iterating over the records is more efficient compared\n",
    "to `iterrows`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "green_tripdata_2019-10.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./resources/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_datetime</th>\n",
       "      <th>lpep_dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-10-01 00:26:02</td>\n",
       "      <td>2019-10-01 00:39:58</td>\n",
       "      <td>112</td>\n",
       "      <td>196</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01 00:18:11</td>\n",
       "      <td>2019-10-01 00:22:38</td>\n",
       "      <td>43</td>\n",
       "      <td>263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-01 00:09:31</td>\n",
       "      <td>2019-10-01 00:24:47</td>\n",
       "      <td>255</td>\n",
       "      <td>228</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-10-01 00:37:40</td>\n",
       "      <td>2019-10-01 00:41:49</td>\n",
       "      <td>181</td>\n",
       "      <td>181</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-10-01 00:08:13</td>\n",
       "      <td>2019-10-01 00:17:56</td>\n",
       "      <td>97</td>\n",
       "      <td>188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.52</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lpep_pickup_datetime lpep_dropoff_datetime  PULocationID  DOLocationID  \\\n",
       "0  2019-10-01 00:26:02   2019-10-01 00:39:58           112           196   \n",
       "1  2019-10-01 00:18:11   2019-10-01 00:22:38            43           263   \n",
       "2  2019-10-01 00:09:31   2019-10-01 00:24:47           255           228   \n",
       "3  2019-10-01 00:37:40   2019-10-01 00:41:49           181           181   \n",
       "4  2019-10-01 00:08:13   2019-10-01 00:17:56            97           188   \n",
       "\n",
       "   passenger_count  trip_distance  tip_amount  \n",
       "0              1.0           5.88        0.00  \n",
       "1              1.0           0.80        0.00  \n",
       "2              2.0           7.50        0.00  \n",
       "3              1.0           0.90        0.00  \n",
       "4              1.0           2.52        2.26  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"./resources/green_tripdata_2019-10.csv.gz\"\n",
    "# Filter the DataFrame to include only the required columns\n",
    "columns = [\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "]\n",
    "# Read the Green 2019-10 CSV file\n",
    "df_green_filtered = pd.read_csv(file_path, compression='gzip', header=0, usecols=columns)\n",
    "df_green_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lpep_pickup_datetime': '2019-10-01 00:26:02', 'lpep_dropoff_datetime': '2019-10-01 00:39:58', 'PULocationID': 112, 'DOLocationID': 196, 'passenger_count': 1.0, 'trip_distance': 5.88, 'tip_amount': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# row._fields returns a tuple of column names, and getattr(row, col) fetches the value of the column for the current row. \n",
    "# So, it creates a dictionary where the keys are column names and the values are the corresponding row values.\n",
    "\n",
    "# Iterate over the records in the DataFrame\n",
    "for row in df_green_filtered.itertuples(index=False):\n",
    "    # Convert the row to a dictionary\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    print(row_dict)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Sending the Trip Data\n",
    "\n",
    "* Create a topic `green-trips` and send the data there\n",
    "* How much time in seconds did it take? (You can round it to a whole number)\n",
    "* Make sure you don't include sleeps in your code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476386, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_green_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 61 seconds\n",
      "total records published 476386\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "\n",
    "# Sending Trip Data to Kafka Topic\n",
    "for row in df_green_filtered.itertuples(index=False):\n",
    "    message = {col: getattr(row, col) for col in row._fields}  # Convert the row to a dictionary\n",
    "    producer.send(topic_name, value=message)  # Send message to Kafka topic\n",
    "    count += 1\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Calculate time taken in seconds\n",
    "time_taken = round(t1 - t0)\n",
    "print(f'Time taken: {time_taken} seconds')\n",
    "print('total records published', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME         PARTITIONS  REPLICAS\r\n",
      "green-trips  1           1\r\n",
      "test-topic   1           1\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti redpanda-1 rpk topic list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY\r\n",
      "=======\r\n",
      "NAME        green-trips\r\n",
      "PARTITIONS  1\r\n",
      "REPLICAS    1\r\n",
      "\r\n",
      "CONFIGS\r\n",
      "=======\r\n",
      "KEY                           VALUE       SOURCE\r\n",
      "cleanup.policy                delete      DYNAMIC_TOPIC_CONFIG\r\n",
      "compression.type              producer    DEFAULT_CONFIG\r\n",
      "max.message.bytes             1048576     DEFAULT_CONFIG\r\n",
      "message.timestamp.type        CreateTime  DEFAULT_CONFIG\r\n",
      "redpanda.remote.delete        true        DEFAULT_CONFIG\r\n",
      "redpanda.remote.read          false       DEFAULT_CONFIG\r\n",
      "redpanda.remote.write         false       DEFAULT_CONFIG\r\n",
      "retention.bytes               -1          DEFAULT_CONFIG\r\n",
      "retention.local.target.bytes  -1          DEFAULT_CONFIG\r\n",
      "retention.local.target.ms     86400000    DEFAULT_CONFIG\r\n",
      "retention.ms                  604800000   DEFAULT_CONFIG\r\n",
      "segment.bytes                 134217728   DEFAULT_CONFIG\r\n",
      "segment.ms                    1209600000  DEFAULT_CONFIG\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti redpanda-1 rpk topic describe green-trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the PySpark consumer\n",
    "\n",
    "Now let's read the data with PySpark.\n",
    "\n",
    "Spark needs a library (jar) to be able to connect to Kafka,\n",
    "so we need to tell PySpark that it needs to use it:\n",
    "\n",
    "```python\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "Now we can connect to the stream:\n",
    "\n",
    "```python\n",
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "In order to test that we can consume from the stream,\n",
    "let's see what will be the first record there.\n",
    "\n",
    "In Spark streaming, the stream is represented as a sequence of\n",
    "small batches, each batch being a small RDD (or a small dataframe).\n",
    "\n",
    "So we can execute a function over each mini-batch.\n",
    "Let's run `take(1)` there to see what do we have in the stream:\n",
    "\n",
    "```python\n",
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()\n",
    "```\n",
    "\n",
    "You should see a record like this:\n",
    "\n",
    "```\n",
    "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 12, 22, 42, 9, 411000), timestampType=0)\n",
    "```\n",
    "\n",
    "Now let's stop the query, so it doesn't keep consuming messages\n",
    "from the stream\n",
    "\n",
    "```python\n",
    "query.stop()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ndirangu749/spark/jdk-11.0.2\n",
      "/home/ndirangu749/spark/spark-3.3.2-bin-hadoop3\n",
      "/home/ndirangu749/spark/spark-3.3.2-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip:/home/ndirangu749/spark/spark-3.3.2-bin-hadoop3/python/:\n"
     ]
    }
   ],
   "source": [
    "!echo $JAVA_HOME\n",
    "!echo $SPARK_HOME\n",
    "!echo $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "print(kafka_jar_package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code sets up a streaming DataFrame (`green_stream`) in Apache Spark structured streaming to consume data from a Kafka topic named \"green-trips\". Here's a breakdown of each part of the code:\n",
    "\n",
    "1. **`spark.readStream`**: This creates a DataFrameReader for reading streaming data. It's used to configure the stream processing.\n",
    "\n",
    "2. **`.format(\"kafka\")`**: Specifies the format of the stream data source. In this case, it's set to \"kafka\" to indicate that the source is Kafka.\n",
    "\n",
    "3. **`.option(\"kafka.bootstrap.servers\", \"localhost:9092\")`**: Sets the Kafka bootstrap servers that Spark will connect to. This is necessary for establishing communication with the Kafka cluster. Here, it's set to localhost on port 9092, assuming Kafka is running on the same machine.\n",
    "\n",
    "4. **`.option(\"subscribe\", \"green-trips\")`**: Specifies the Kafka topic to subscribe to. The streaming DataFrame will consume data from the \"green-trips\" topic.\n",
    "\n",
    "5. **`.option(\"startingOffsets\", \"earliest\")`**: Sets the starting point for reading data from the Kafka topic. \"earliest\" means that it will start reading from the earliest available offset in the topic, ensuring that all existing data in the topic is consumed.\n",
    "\n",
    "6. **`.load()`**: Loads the streaming data as a DataFrame. This establishes the connection to Kafka and initializes the stream, but it doesn't start processing data yet.\n",
    "\n",
    "After executing this code, `green_stream` will represent a streaming DataFrame that continuously reads data from the \"green-trips\" Kafka topic. Further operations or transformations can be applied to this DataFrame to process the incoming data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f91b275f760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 22, 7, 25, 51, 890000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    \"\"\"\n",
    "    Function to print the first row of each micro-batch.\n",
    "    \n",
    "    Parameters:\n",
    "        mini_batch (DataFrame): A DataFrame containing a micro-batch of data.\n",
    "        batch_id (int): The ID of the current batch.\n",
    "    \"\"\"\n",
    "    # Take the first row from the micro-batch DataFrame\n",
    "    first_row = mini_batch.take(1)\n",
    "    \n",
    "    # Check if the micro-batch contains at least one row\n",
    "    if first_row:\n",
    "        # Print the first row of the micro-batch\n",
    "        print(first_row[0])\n",
    "\n",
    "# Set up the streaming query to apply the peek function to each micro-batch\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming query has stopped\n"
     ]
    }
   ],
   "source": [
    "if query.isActive:\n",
    "    print(\"Streaming query is active\")\n",
    "else:\n",
    "    print(\"Streaming query has stopped\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Parsing the data\n",
    "\n",
    "The data is JSON, but currently it's in binary format. We need\n",
    "to parse it and turn it into a streaming dataframe with proper\n",
    "columns.\n",
    "\n",
    "Similarly to PySpark, we define the schema\n",
    "\n",
    "```python\n",
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())\n",
    "```\n",
    "\n",
    "And apply this schema:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")\n",
    "```\n",
    "\n",
    "How does the record look after parsing? Copy the output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = green_stream \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24/03/22 07:33:25 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query\n",
    "            didn't fail: /tmp/temporary-f22476b2-c089-458e-bc2b-035700528cb9. If it's required to delete it under any\n",
    "            circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to\n",
    "            know deleting temp checkpoint folder is best effort.\n",
    "24/03/22 07:33:25 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets\n",
    "            and will be disabled.\n",
    "-------------------------------------------                                     \n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
    "|lpep_pickup_datetime|lpep_dropoff_datetime|PULocationID|DOLocationID|passenger_count|trip_distance|tip_amount|\n",
    "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
    "| 2019-10-01 00:26:02|  2019-10-01 00:39:58|         112|         196|            1.0|         5.88|       0.0|\n",
    "| 2019-10-01 00:18:11|  2019-10-01 00:22:38|          43|         263|            1.0|          0.8|       0.0|\n",
    "| 2019-10-01 00:09:31|  2019-10-01 00:24:47|         255|         228|            2.0|          7.5|       0.0|\n",
    "| 2019-10-01 00:37:40|  2019-10-01 00:41:49|         181|         181|            1.0|          0.9|       0.0|\n",
    "| 2019-10-01 00:08:13|  2019-10-01 00:17:56|          97|         188|            1.0|         2.52|      2.26|\n",
    "| 2019-10-01 00:35:01|  2019-10-01 00:43:40|          65|          49|            1.0|         1.47|      1.86|\n",
    "| 2019-10-01 00:28:09|  2019-10-01 00:30:49|           7|         179|            1.0|          0.6|       1.0|\n",
    "| 2019-10-01 00:28:26|  2019-10-01 00:32:01|          41|          74|            1.0|         0.56|       0.0|\n",
    "| 2019-10-01 00:14:01|  2019-10-01 00:26:16|         255|          49|            1.0|         2.42|       0.0|\n",
    "| 2019-10-01 00:03:03|  2019-10-01 00:17:13|         130|         131|            1.0|          3.4|      2.85|\n",
    "| 2019-10-01 00:07:10|  2019-10-01 00:23:38|          24|          74|            3.0|         3.18|       0.0|\n",
    "| 2019-10-01 00:25:48|  2019-10-01 00:49:52|         255|         188|            1.0|          4.7|       1.0|\n",
    "| 2019-10-01 00:03:12|  2019-10-01 00:14:43|         129|         160|            1.0|          3.1|       0.0|\n",
    "| 2019-10-01 00:44:56|  2019-10-01 00:51:06|          18|         169|            1.0|         1.19|      0.25|\n",
    "| 2019-10-01 00:55:14|  2019-10-01 01:00:49|         223|           7|            1.0|         1.09|      1.46|\n",
    "| 2019-10-01 00:06:06|  2019-10-01 00:11:05|          75|         262|            1.0|         1.24|      2.01|\n",
    "| 2019-10-01 00:00:19|  2019-10-01 00:14:32|          97|         228|            1.0|         3.03|      3.58|\n",
    "| 2019-10-01 00:09:31|  2019-10-01 00:20:41|          41|          74|            1.0|         2.03|      2.16|\n",
    "| 2019-10-01 00:30:36|  2019-10-01 00:34:30|          41|          42|            1.0|         0.73|      1.26|\n",
    "| 2019-10-01 00:58:32|  2019-10-01 01:05:08|          41|         116|            1.0|         1.48|       0.0|\n",
    "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Most popular destination\n",
    "\n",
    "Now let's finally do some streaming analytics. We will\n",
    "see what's the most popular destination currently\n",
    "based on our stream of data (which ideally we should\n",
    "have sent with delays like we did in workshop 2)\n",
    "\n",
    "\n",
    "This is how you can do it:\n",
    "\n",
    "* Add a column \"timestamp\" using the `current_timestamp` function\n",
    "* Group by:\n",
    "  * 5 minutes window based on the timestamp column (`F.window(col(\"timestamp\"), \"5 minutes\")`)\n",
    "  * `\"DOLocationID\"`\n",
    "* Order by count\n",
    "\n",
    "You can print the output to the console using this\n",
    "code\n",
    "\n",
    "```python\n",
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "Write the most popular destination, your answer should be *either* the zone ID or the zone name of this destination. (You will need to re-send the data for this to work)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Add a timestamp column\n",
    "stream_with_timestamp = green_stream.withColumn(\"timestamp\", F.current_timestamp())\n",
    "\n",
    "# Group by 5-minute window and DOLocationID, count occurrences, and order by count\n",
    "popular_destinations = stream_with_timestamp \\\n",
    "    .groupBy(F.window(\"timestamp\", \"5 minutes\"), \"DOLocationID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "# Start the streaming query and write the output to the console\n",
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24/03/22 07:41:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query\n",
    "            didn't fail: /tmp/temporary-87f64411-070e-45a0-b5e9-42f4bca67f45. If it's required to delete it under any \n",
    "            circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know\n",
    "            deleting temp checkpoint folder is best effort.\n",
    "24/03/22 07:41:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets \n",
    "            and will be disabled.\n",
    "-------------------------------------------                                     \n",
    "Batch: 0\n",
    "-------------------------------------------\n",
    "+------------------------------------------+------------+-----+\n",
    "|window                                    |DOLocationID|count|\n",
    "+------------------------------------------+------------+-----+\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|74          |17741|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|42          |15942|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|41          |14061|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|75          |12840|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|129         |11930|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|7           |11533|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|166         |10845|\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|236         |7913 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|223         |7542 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|238         |7318 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|82          |7292 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|181         |7282 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|95          |7244 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|244         |6733 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|61          |6606 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|116         |6339 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|138         |6144 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|97          |6050 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|49          |5221 |\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|151         |5153 |\n",
    "+------------------------------------------+------------+-----+\n",
    "only showing top 20 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+------------------------------------------+------------+-----+\n",
    "|window                                    |DOLocationID|count|\n",
    "+------------------------------------------+------------+-----+\n",
    "|{2024-03-22 07:40:00, 2024-03-22 07:45:00}|74          |17741|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
